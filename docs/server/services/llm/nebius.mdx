---
title: "Nebius"
description: "LLM service implementation using Nebius AI Studio's API with OpenAI-compatible interface"
---

## Overview

`NebiusLLMService` provides access to Nebius AI Studio's language models through an OpenAI-compatible interface. It inherits from `OpenAILLMService` and supports streaming responses, function calling, and context management.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.nebius.llm.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Nebius Docs"
    icon="book"
    href="https://docs.nebius.ai/ai/api/v1/llm/api-reference"
  >
    Official Nebius AI Studio API documentation and features
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14x-function-calling-nebius.py"
  >
    Working example with function calling
  </Card>
</CardGroup>

## Installation

To use Nebius services, install the required dependency:

```bash
pip install "pipecat-ai[nebius]"
```

You'll also need to set up your Nebius API key as an environment variable: `NEBIUS_API_KEY`.

<Tip>
  Get your API key from [Nebius AI Studio Console](https://studio.nebius.ai/).
</Tip>

## Frames

### Input

- `OpenAILLMContextFrame` - Conversation context and history
- `LLMMessagesFrame` - Direct message list
- `VisionImageRawFrame` - Images for vision processing (select models)
- `LLMUpdateSettingsFrame` - Runtime parameter updates

### Output

- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - Response boundaries
- `LLMTextFrame` - Streamed completion chunks
- `FunctionCallInProgressFrame` / `FunctionCallResultFrame` - Function call lifecycle
- `ErrorFrame` - API or processing errors

## Function Calling

<Card
  title="Function Calling Guide"
  icon="function"
  href="/learn/function-calling"
>
  Learn how to implement function calling with standardized schemas, register
  handlers, manage context properly, and control execution flow in your
  conversational AI applications.
</Card>

## Context Management

<Card
  title="Context Management Guide"
  icon="messages"
  href="/learn/context-management"
>
  Learn how to manage conversation context, handle message history, and
  integrate context aggregators for consistent conversational experiences.
</Card>

## Usage Example

```python
import os
from pipecat.services.nebius.llm import NebiusLLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema

# Configure Nebius service with default model
llm = NebiusLLMService(
    api_key=os.getenv("NEBIUS_API_KEY"),
    model="meta-llama/Meta-Llama-3.1-8B-Instruct-fast",  # Default fast model
    params=NebiusLLMService.InputParams(
        temperature=0.7,
        top_p=0.9,
        max_tokens=1000
    )
)

# Set up conversation context
messages = [
    {
        "role": "system", 
        "content": "You are a helpful assistant powered by Nebius AI Studio."
    }
]

# Optional: Add function calling capabilities
tools = ToolsSchema([
    FunctionSchema(
        name="get_current_weather",
        description="Get the current weather in a given location",
        properties={
            "location": {
                "type": "string",
                "description": "The city and state, e.g. San Francisco, CA"
            },
            "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
        },
        required=["location"]
    )
])

context = OpenAILLMContext(messages, tools)
from pipecat.processors.aggregators.llm_response import LLMUserAggregatorParams

context_aggregator = llm.create_context_aggregator(
    context,
    user_params=LLMUserAggregatorParams(aggregation_timeout=0.1)
)

# Register function handler
async def fetch_weather(params):
    location = params.arguments["location"]
    await params.result_callback({"conditions": "sunny", "temperature": "22Â°C"})

llm.register_function("get_current_weather", fetch_weather)

# Optional: Add function call feedback for better UX
@llm.event_handler("on_function_calls_started")
async def on_function_calls_started(service, function_calls):
    await tts.queue_frame(TTSSpeakFrame("Let me check that for you."))

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,  # Your preferred STT service
    context_aggregator.user(),
    llm,
    tts,  # Your preferred TTS service
    transport.output(),
    context_aggregator.assistant()
])
```

## Available Models

Nebius AI Studio provides access to various state-of-the-art models:

- `meta-llama/Meta-Llama-3.1-8B-Instruct-fast` - Default fast model (recommended)
- `meta-llama/Meta-Llama-3.1-70B-Instruct` - Larger model for complex tasks
- `meta-llama/Meta-Llama-3.1-405B-Instruct` - Most capable model

<Tip>
  Check the [Nebius AI Studio Console](https://studio.nebius.ai/) for the latest available models and pricing.
</Tip>

## Configuration

```python
# Custom configuration example
llm = NebiusLLMService(
    api_key=os.getenv("NEBIUS_API_KEY"),
    base_url="https://api.studio.nebius.ai/v1",  # Default base URL
    model="meta-llama/Meta-Llama-3.1-70B-Instruct",
    params=NebiusLLMService.InputParams(
        temperature=0.3,  # Lower temperature for more focused responses
        top_p=0.8,
        max_tokens=2048,
        frequency_penalty=0.1
    )
)
```

## Metrics

Inherits all OpenAI metrics capabilities:

- **Time to First Byte (TTFB)** - Response latency measurements
- **Processing Duration** - Model processing times
- **Token Usage** - Prompt tokens, completion tokens, and totals

<Info>
  [Learn how to enable Metrics](/guides/fundamentals/metrics) in your Pipeline.
</Info>

## Additional Notes

- **OpenAI Compatibility**: Full compatibility with OpenAI API features and parameters
- **High Performance**: Optimized for low-latency conversational AI applications
- **Enterprise Ready**: Built on Nebius cloud infrastructure for reliability and scale
- **Cost Effective**: Competitive pricing for high-quality language models
- **Multi-language Support**: Models support multiple languages and regions 